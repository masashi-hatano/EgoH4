<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description"
    content="This is a project page for The Invisible EgoHand: 3D Hand Forecasting through EgoBody Pose Estimation">
  <meta property="og:title" content="The Invisible EgoHand: 3D Hand Forecasting through EgoBody Pose Estimation">
  <meta property="og:description"
    content="This is a project page for The Invisible EgoHand: 3D Hand Forecasting through EgoBody Pose Estimation">
  <meta property="og:url" content="https://masashi-hatano.github.io/EgoH4/">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <!-- <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/> -->


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="masashi, hatano, keio, egocentric, egoh4, hand, forecasting, body, pose">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>The Invisible EgoHand: 3D Hand Forecasting through EgoBody Pose Estimation</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <!-- Add Splide CSS (for styling the carousel) -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@splidejs/splide@3.6.12/dist/css/splide.min.css">

  <!-- Add Splide JS (for functionality) -->
  <script src="https://cdn.jsdelivr.net/npm/@splidejs/splide@3.6.12/dist/js/splide.min.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">The Invisible EgoHand: 3D Hand Forecasting through EgoBody Pose
              Estimation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://masashi-hatano.github.io/" target="_blank">Masashi Hatano</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://zhifanzhu.github.io/" target="_blank">Zhifan Zhu</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.co.jp/citations?user=JU9x-bcAAAAJ&hl=en&oi=ao" target="_blank">Hideo
                  Saito</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://dimadamen.github.io/" target="_blank">Dima Damen</a><sup>2</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup> Keio University, <sup>2</sup> University of Bristol<br></span>
              <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
            </div>

            <!-- <div class="is-size-4">
              <b><span class="author-block">Under Review</span></b>
            </div> -->

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://masashi-hatano.github.io/assets/pdf/egoh4.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary PDF link -->
                <span class="link-block">
                  <a href="https://masashi-hatano.github.io/assets/pdf/egoh4_supp.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Supplementary</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/masashi-hatano/EgoH4" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2504.08654" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                <!-- YouTube Video Link -->
                <!-- <span class="link-block">
                  <a href="https://youtu.be/f4ZK66xI3LU" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span> -->

                <!-- Data Link -->
                <!-- <span class="link-block">
                  <a href="https://huggingface.co/datasets/masashi-hatano/MM-CDFSL/tree/main" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa fa-database"></i>
                    </span>
                    <span>Data</span>
                  </a>
                </span> -->

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Teaser video-->
  <!-- <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
  <!-- Your video here -->
  <!-- <source src="static/videos/supplementary.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
        </h2>
      </div>
    </div>
  </section> -->
  <!-- End teaser video -->

  <!-- Teaser -->
  <section class="hero teaser is-small">
    <div class="container is-max-desktop">
      <div style="text-align: center;">
        <img src="static/images/teaser.jpg" style="width: 50%; height: auto;" alt="MY ALT TEXT" />
        <div align="left"
          style="position: relative; left: 10%; width: 80%; height: auto; margin-top: 20px; margin-bottom: 50px;">
          <figcaption>Given signals during observation: camera poses, images,
            and visible hand locations in 2D, our proposed method <b>EgoH4</b>
            forecasts future 3D hand pose. EgoH4 can forecast hand joints
            even when hands are out of view during observation. We show
            visible 2D hand positions overlaid on the observation frames <i>t</i><sub>1</sub>
            and <i>t</i><sub>2</sub>, and the corresponding camera poses attached on the heads.
            At <i>t</i><sub>2</sub>, the right hand is invisible. In the forecasting frame, the right
            hand is back in view while the left hand is now out of view.</figcaption>
        </div>
      </div>
    </div>
  </section>
  <!-- End image Approach -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Forecasting hand motion and pose from an egocentric perspective is essential for understanding human
              intention.
              However, existing methods focus solely on predicting positions without considering articulation, and only
              when the hands are visible in the field of view.
              This limitation overlooks the fact that approximate hand positions can still be inferred even when they
              are outside the camera's view.
              In this paper, we propose a method to forecast the 3D trajectories and poses of both hands from an
              egocentric video, both in and out of the field of view.
              We propose a diffusion-based transformer architecture for Egocentric Hand Forecasting, EgoH4,
              which takes
              as input the observation sequence and camera poses, then predicts future 3D motion and poses for both
              hands of the camera wearer.
              We leverage full-body pose information, allowing other joints to provide constraints on hand motion.
              We denoise the hand and body joints along with a visibility predictor for hand joints and a 3D-to-2D
              reprojection loss that minimizes the error when hands are in-view.
              We evaluate EgoH4 on the Ego-Exo4D dataset, combining subsets with body and hand annotations.
              We train on 156K sequences and evaluate on 34K sequences, respectively.
              EgoH4 improves the performance by 3.4cm and 5.1cm over the baseline in terms of ADE for hand trajectory
              forecasting and MPJPE for hand pose forecasting.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->


  <!-- Image Approach -->
  <section class="section hero is-small">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Our Method, EgoH4</h2>
      <div style="text-align: center;">
        <img src="static/images/overview.jpg" style="width: 80%; height: auto;" alt="MY ALT TEXT" />
        <div align="left" style="margin-top: 20px;">
          <figcaption><b>The framework of our proposed method, EgoH4</b>. We show the denoising network in a single
            denoising step. During training, we estimate the original data &#119909;<sub>0</sub> from an arbitrary noise
            level n to learn the denoising network. During inference, we iteratively denoise the noisy joints over the
            maximum diffusion step N from N to 0.</figcaption>
        </div>
      </div>
    </div>
  </section>
  <!-- End image Approach -->


  <!-- Carousel structure -->
  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container is-max-desktop" style="text-align: center;">
        <h2 class="title">Qualitative Results</h2>
        <h2 class="title">Hand Trajectory Forecasting</h2>
        <div class="splide" id="splide1">
          <div class="splide__track">
            <ul class="splide__list" style="width: 60%;">
              <li class="splide__slide">
                <video poster="" id="video1" autoplay controls muted loop playsinline>
                  <source src="static/videos/traj_in_covid.mp4" type="video/mp4">
                </video>
              </li>
              <li class="splide__slide">
                <video poster="" id="video1" autoplay controls muted loop playsinline>
                  <source src="static/videos/traj_in_cooking.mp4" type="video/mp4">
                </video>
              </li>
              <li class="splide__slide">
                <video poster="" id="video1" autoplay controls muted loop playsinline>
                  <source src="static/videos/traj_out_basket.mp4" type="video/mp4">
                </video>
              </li>
              <li class="splide__slide">
                <video poster="" id="video1" autoplay controls muted loop playsinline>
                  <source src="static/videos/traj_out_dance.mp4" type="video/mp4">
                </video>
              </li>
            </ul>
          </div>
        </div>
        <p style="text-align: left;">
          <b>Qualitative results for hand trajectory forecasting</b>. We show sample qualitative resutls across
          activities: cooking, covid testing, basketball, and dance exercises. Dots in <font color="#ff0000">red</font>,
          <font color="#228b22">green</font>, <font color="#0000ff">blue</font>, <font color="#800080">purple</font>,
          and <font color="#ffa500">orange</font> represent the prediction of left/right future hands, ground-truth of
          lef/right hands, and the prediction of body joints at the last observable frame, respectively. For each track
          darker colors indicate later times.
        </p>
      </div>
    </div>

    <div class="hero-body">
      <div class="container is-max-desktop" style="text-align: center;">
        <h2 class="title">Hand Pose Forecasting</h2>
        <div class="splide" id="splide2">
          <div class="splide__track">
            <ul class="splide__list" style="width: 60%;">
              <li class="splide__slide">
                <video poster="" id="video1" autoplay controls muted loop playsinline>
                  <source src="static/videos/pose_bike.mp4" type="video/mp4">
                </video>
              </li>
              <li class="splide__slide">
                <video poster="" id="video1" autoplay controls muted loop playsinline>
                  <source src="static/videos/pose_cooking.mp4" type="video/mp4">
                </video>
              </li>
              <li class="splide__slide">
                <video poster="" id="video1" autoplay controls muted loop playsinline>
                  <source src="static/videos/pose_covid.mp4" type="video/mp4">
                </video>
              </li>
              <li class="splide__slide">
                <video poster="" id="video1" autoplay controls muted loop playsinline>
                  <source src="static/videos/pose_piano.mp4" type="video/mp4">
                </video>
            </ul>
          </div>
        </div>
        <p style="text-align: left;">
          <b>Qualitative results for hand pose forecasting</b>. Dots in <font color="#ff0000">red</font> and
          <font color="#228b22">green</font> denote the prediction and ground-truth, respectively. Note that we expand
          the image plane so that we can also show the out-of-view hands.
        </p>
      </div>
    </div>
  </section>

  <!-- Youtube video -->
  <!-- <section class="section hero is-samll is-light">
    <div calss="hero body">
      <div class="container is-max-desktop">
        <center>
          <h2 class="title is-3">Video Presentation</h2>
          <div class="column is-four-fifths">

            <div class="publication-video">
              <iframe src="https://www.youtube.com/embed/f4ZK66xI3LU?rel=0&showinfo=0" frameborder="0"
                allow="autoplay; encrypted-media" allowfullscreen></iframe>
            </div>
        </center>
      </div>
    </div>
    </div>
  </section> -->
  <!-- End youtube video -->

  <!-- Paper poster -->
  <!-- <section class="section is-small">
    <div class="container is-max-desktop">

      <h2 class="title">Poster</h2>

      <div style="position: relative; width: 100%; height: calc(100vw * 0.48); max-height: 550px; overflow: hidden;">
        <iframe src="static/pdfs/mm-cdfsl_poster.pdf"
          style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border: none;" allowfullscreen>
        </iframe>
      </div>

    </div>
  </section> -->
  <!--End paper poster -->

  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @article{Hatano2025EgoH4,
          author = {Hatano, Masashi and Zhu, Zhifan and Saito, Hideo and Damen, Dima},
          title = {The Invisible EgoHand: 3D Hand Forecasting through EgoBody Pose Estimation},
          journal = {arXiv preprint arXiv:2504.08654},
          year = {2025},
        }
      </code></pre>
    </div>
  </section>
  <!--End BibTex citation -->

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic
                Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io"
                target="_blank">Nerfies</a> project page.
              You are free to borrow the of this website, we just ask that you link back to this page in the
              footer.
              <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

  <script>
    // Initialize the first Splide carousel
    var splide1 = new Splide('#splide1', {
      type: 'loop',
      padding: '5rem',
    });
    splide1.mount();

    // Initialize the second Splide carousel
    var splide2 = new Splide('#splide2', {
      type: 'loop',
      padding: '5rem',
    });
    splide2.mount();
  </script>


</body>

</html>